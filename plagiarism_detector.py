#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åæŠ„è¢­æ£€æµ‹ç¨‹åº
æ£€æµ‹ä»£ç ç›¸ä¼¼åº¦å’Œå…¶ä»–ç‰¹å¾ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import re
import hashlib
import difflib
import json
import time
import sys
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class PlagiarismDetector:
    def __init__(self):
        self.results = {}
        self.similarity_matrix = {}
        self.suspicious_pairs = []
        self.special_keywords = ['freopen', 'system', 'exec', 'eval', 'subprocess', '//', '/*', '*/']
        
    def normalize_code(self, code: str) -> str:
        """æ ‡å‡†åŒ–ä»£ç ï¼Œç§»é™¤æ³¨é‡Šã€ç©ºè¡Œã€å¤šä½™ç©ºæ ¼ç­‰"""
        # ç§»é™¤C++é£æ ¼æ³¨é‡Š
        code = re.sub(r'//.*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œç©ºè¡Œ
        lines = [line.strip() for line in code.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def extract_features(self, code: str) -> Dict:
        """æå–ä»£ç ç‰¹å¾"""
        normalized_code = self.normalize_code(code)
        
        features = {
            'length': len(normalized_code),
            'lines': len(code.split('\n')),
            'variables': self.extract_variables(normalized_code),
            'functions': self.extract_functions(normalized_code),
            'keywords': self.extract_keywords(normalized_code),
            'structure': self.extract_structure(normalized_code),
            'hash': hashlib.md5(normalized_code.encode()).hexdigest(),
            'normalized_code': normalized_code,
            'special_keywords': self.detect_special_keywords(code)
        }
        
        return features
    
    def extract_variables(self, code: str) -> List[str]:
        """æå–å˜é‡å"""
        # åŒ¹é…å˜é‡å£°æ˜
        var_patterns = [
            r'\b(int|long|double|float|char|bool|string)\s+([a-zA-Z_][a-zA-Z0-9_]*)\b',
            r'\b(vector|map|set|queue|stack)\s*<[^>]*>\s+([a-zA-Z_][a-zA-Z0-9_]*)\b',
            r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*[=;]\s*'
        ]
        
        variables = set()
        for pattern in var_patterns:
            matches = re.findall(pattern, code)
            for match in matches:
                if isinstance(match, tuple):
                    variables.update(match)
                else:
                    variables.add(match)
        
        return list(variables)
    
    def extract_functions(self, code: str) -> List[str]:
        """æå–å‡½æ•°å"""
        func_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\([^)]*\)\s*\{'
        functions = re.findall(func_pattern, code)
        return functions
    
    def extract_keywords(self, code: str) -> Dict[str, int]:
        """æå–å…³é”®å­—é¢‘ç‡"""
        keywords = [
            'if', 'else', 'for', 'while', 'do', 'switch', 'case', 'break', 'continue',
            'return', 'cin', 'cout', 'printf', 'scanf', 'main', 'include', 'using',
            'namespace', 'std', 'vector', 'map', 'set', 'queue', 'stack', 'struct',
            'class', 'public', 'private', 'protected', 'const', 'static', 'void',
            'int', 'long', 'double', 'float', 'char', 'bool', 'string'
        ]
        
        keyword_count = {}
        for keyword in keywords:
            count = len(re.findall(r'\b' + keyword + r'\b', code, re.IGNORECASE))
            if count > 0:
                keyword_count[keyword] = count
        
        return keyword_count
    
    def extract_structure(self, code: str) -> Dict:
        """æå–ä»£ç ç»“æ„ç‰¹å¾"""
        structure = {
            'brackets': code.count('{') + code.count('}'),
            'parentheses': code.count('(') + code.count(')'),
            'semicolons': code.count(';'),
            'if_statements': len(re.findall(r'\bif\s*\(', code)),
            'for_loops': len(re.findall(r'\bfor\s*\(', code)),
            'while_loops': len(re.findall(r'\bwhile\s*\(', code)),
            'function_calls': len(re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\s*\(', code))
        }
        return structure
    

    
    def detect_special_keywords(self, code: str) -> Dict[str, List[str]]:
        """æ£€æµ‹ç‰¹æ®Šå…³é”®è¯å’Œæ³¨é‡Š"""
        detected = {}
        
        # æ£€æµ‹ç‰¹æ®Šå…³é”®è¯
        for keyword in self.special_keywords:
            if keyword in ['//', '/*', '*/']:
                # å¯¹äºæ³¨é‡Šç¬¦å·ï¼Œæå–å®Œæ•´çš„æ³¨é‡Šå†…å®¹
                if keyword == '//':
                    # å•è¡Œæ³¨é‡Š
                    comment_lines = re.findall(r'//(.*)$', code, re.MULTILINE)
                    if comment_lines:
                        lines = code.split('\n')
                        keyword_lines = []
                        for i, line in enumerate(lines, 1):
                            if '//' in line:
                                comment_start = line.find('//')
                                comment_content = line[comment_start:].strip()
                                keyword_lines.append(f"ç¬¬{i}è¡Œ: {comment_content}")
                        detected['å•è¡Œæ³¨é‡Š'] = keyword_lines
                elif keyword == '/*':
                    # å¤šè¡Œæ³¨é‡Šå¼€å§‹
                    multi_comments = re.findall(r'/\*(.*?)\*/', code, re.DOTALL)
                    if multi_comments:
                        lines = code.split('\n')
                        keyword_lines = []
                        for i, line in enumerate(lines, 1):
                            if '/*' in line or '*/' in line:
                                keyword_lines.append(f"ç¬¬{i}è¡Œ: {line.strip()}")
                        detected['å¤šè¡Œæ³¨é‡Š'] = keyword_lines
            else:
                # å…¶ä»–ç‰¹æ®Šå…³é”®è¯
                matches = re.findall(rf'\b{keyword}\b', code, re.IGNORECASE)
                if matches:
                    # è·å–åŒ…å«å…³é”®è¯çš„å®Œæ•´è¡Œ
                    lines = code.split('\n')
                    keyword_lines = []
                    for i, line in enumerate(lines, 1):
                        if keyword.lower() in line.lower():
                            keyword_lines.append(f"ç¬¬{i}è¡Œ: {line.strip()}")
                    detected[keyword] = keyword_lines
        
        return detected
    
    def calculate_similarity(self, code1: str, code2: str) -> float:
        """è®¡ç®—ä»£ç ç›¸ä¼¼åº¦"""
        # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, code1, code2).ratio()
        return similarity
    
    def calculate_tfidf_similarity(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨TF-IDFè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        vectorizer = TfidfVectorizer(
            analyzer='char',
            ngram_range=(3, 5),
            min_df=1,
            max_df=0.9
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            similarity_matrix = cosine_similarity(tfidf_matrix)
            return similarity_matrix
        except:
            # å¦‚æœTF-IDFå¤±è´¥ï¼Œè¿”å›é›¶çŸ©é˜µ
            return np.zeros((len(texts), len(texts)))
    
    def detect_plagiarism(self, contest_name: str) -> Dict:
        """æ£€æµ‹æŠ„è¢­"""
        contest_dir = os.path.join("result", contest_name)
        if not os.path.exists(contest_dir):
            print(f"é”™è¯¯: æ¯”èµ›ç›®å½• {contest_dir} ä¸å­˜åœ¨")
            return {}
        
        print(f"å¼€å§‹æ£€æµ‹æ¯”èµ›: {contest_name}")
        
        # è·å–æ‰€æœ‰å­ç›®å½•ï¼ˆé¢˜ç›®ï¼‰
        subdirs = []
        for item in os.listdir(contest_dir):
            subdir_path = os.path.join(contest_dir, item)
            if os.path.isdir(subdir_path):
                subdirs.append(subdir_path)
        
        if not subdirs:
            print(f"åœ¨ {contest_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°é¢˜ç›®ç›®å½•")
            return {}
        
        all_reports = {}
        
        # æ£€æµ‹æ¯ä¸ªé¢˜ç›®
        for subdir in subdirs:
            problem_name = os.path.basename(subdir)
            print(f"æ£€æµ‹é¢˜ç›®: {problem_name}")
            
            # è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶
            code_files = []
            for root, dirs, files in os.walk(subdir):
                for file in files:
                    if file.endswith('.cpp'):
                        file_path = os.path.join(root, file)
                        code_files.append(file_path)
            
            if not code_files:
                print(f"åœ¨ {subdir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»£ç æ–‡ä»¶")
                continue
            
            print(f"æ‰¾åˆ° {len(code_files)} ä¸ªä»£ç æ–‡ä»¶")
            
            # åˆ†ææ¯ä¸ªæ–‡ä»¶
            file_features = {}
            file_codes = {}
            
            for file_path in code_files:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        code = f.read()
                    
                    filename = os.path.basename(file_path)
                    features = self.extract_features(code)
                    file_features[filename] = features
                    file_codes[filename] = features['normalized_code']
                    
                    print(f"åˆ†ææ–‡ä»¶: {filename}")
                    
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            filenames = list(file_codes.keys())
            codes = list(file_codes.values())
            
            # ä½¿ç”¨TF-IDFè®¡ç®—ç›¸ä¼¼åº¦
            tfidf_similarity = self.calculate_tfidf_similarity(codes)
            
            # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ç›¸ä¼¼åº¦
            similarity_pairs = []
            suspicious_pairs = []
            
            for i in range(len(filenames)):
                for j in range(i + 1, len(filenames)):
                    file1, file2 = filenames[i], filenames[j]
                    code1, code2 = codes[i], codes[j]
                    
                    # å¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
                    sequence_similarity = self.calculate_similarity(code1, code2)
                    tfidf_sim = tfidf_similarity[i][j]
                    
                    # ç‰¹å¾ç›¸ä¼¼åº¦
                    features1 = file_features[file1]
                    features2 = file_features[file2]
                    
                    # å˜é‡åç›¸ä¼¼åº¦
                    var_similarity = self.calculate_set_similarity(
                        set(features1['variables']), 
                        set(features2['variables'])
                    )
                    
                    # å‡½æ•°åç›¸ä¼¼åº¦
                    func_similarity = self.calculate_set_similarity(
                        set(features1['functions']), 
                        set(features2['functions'])
                    )
                    
                    # ç»“æ„ç›¸ä¼¼åº¦
                    structure_similarity = self.calculate_structure_similarity(
                        features1['structure'], 
                        features2['structure']
                    )
                    
                    # ç»¼åˆç›¸ä¼¼åº¦
                    overall_similarity = (
                        sequence_similarity * 0.4 +
                        tfidf_sim * 0.3 +
                        var_similarity * 0.1 +
                        func_similarity * 0.1 +
                        structure_similarity * 0.1
                    )
                    
                    similarity_info = {
                        'file1': file1,
                        'file2': file2,
                        'sequence_similarity': sequence_similarity,
                        'tfidf_similarity': tfidf_sim,
                        'variable_similarity': var_similarity,
                        'function_similarity': func_similarity,
                        'structure_similarity': structure_similarity,
                        'overall_similarity': overall_similarity
                    }
                    
                    similarity_pairs.append(similarity_info)
                    
                    # æ ‡è®°å¯ç–‘çš„æŠ„è¢­å¯¹
                    if overall_similarity > 0.5:
                        suspicious_pairs.append(similarity_info)
            
            # ç”Ÿæˆè¯¥é¢˜ç›®çš„æŠ¥å‘Š
            problem_report = {
                'problem_name': problem_name,
                'total_files': len(filenames),
                'file_features': file_features,
                'similarity_pairs': similarity_pairs,
                'suspicious_pairs': suspicious_pairs,
                'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            all_reports[problem_name] = problem_report
        
        # ç”Ÿæˆæ€»æŠ¥å‘Š
        report = {
            'contest_name': contest_name,
            'problems': all_reports,
            'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return report
    
    def calculate_set_similarity(self, set1: Set, set2: Set) -> float:
        """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦"""
        if not set1 and not set2:
            return 1.0
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        return intersection / union if union > 0 else 0.0
    
    def calculate_structure_similarity(self, struct1: Dict, struct2: Dict) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦"""
        if not struct1 or not struct2:
            return 0.0
        
        total_diff = 0
        total_sum = 0
        
        for key in struct1:
            if key in struct2:
                val1, val2 = struct1[key], struct2[key]
                max_val = max(val1, val2)
                if max_val > 0:
                    diff = abs(val1 - val2)
                    total_diff += diff
                    total_sum += max_val
        
        if total_sum == 0:
            return 1.0
        
        return 1.0 - (total_diff / total_sum)
    
    def generate_report(self, report: Dict, contest_name: str):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = os.path.join("result", contest_name, f"{contest_name}_plagiarism_report.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"åæŠ„è¢­æ£€æµ‹æŠ¥å‘Š - {contest_name}\n")
            f.write("=" * 80 + "\n")
            f.write(f"æ£€æµ‹æ—¶é—´: {report['analysis_time']}\n")
            f.write("=" * 80 + "\n\n")
            
            # æ£€æµ‹ç‰¹æ®Šå…³é”®è¯
            special_keywords_found = []
            for problem_name, problem_report in report['problems'].items():
                for filename, features in problem_report['file_features'].items():
                    if features['special_keywords']:
                        special_keywords_found.append({
                            'problem': problem_name,
                            'file': filename,
                            'keywords': features['special_keywords']
                        })
            
            if special_keywords_found:
                f.write("ğŸš¨ æ£€æµ‹åˆ°ç‰¹æ®Šå…³é”®è¯:\n")
                f.write("-" * 50 + "\n")
                for item in special_keywords_found:
                    f.write(f"é¢˜ç›®: {item['problem']}\n")
                    f.write(f"æ–‡ä»¶: {item['file']}\n")
                    for keyword, lines in item['keywords'].items():
                        f.write(f"å…³é”®è¯ '{keyword}' å‡ºç°åœ¨:\n")
                        for line in lines:
                            f.write(f"  {line}\n")
                    f.write("-" * 30 + "\n")
                f.write("\n")
            
            # æ”¶é›†æ‰€æœ‰å¯ç–‘æŠ„è¢­å¯¹
            all_suspicious_pairs = []
            for problem_name, problem_report in report['problems'].items():
                for pair in problem_report['suspicious_pairs']:
                    pair_with_problem = pair.copy()
                    pair_with_problem['problem'] = problem_name
                    all_suspicious_pairs.append(pair_with_problem)
            
            # æ˜¾ç¤ºæ‰€æœ‰å¯ç–‘æŠ„è¢­å¯¹
            if all_suspicious_pairs:
                f.write("ğŸš¨ æ‰€æœ‰å¯ç–‘æŠ„è¢­å¯¹:\n")
                f.write("-" * 50 + "\n")
                for pair in sorted(all_suspicious_pairs, 
                                 key=lambda x: x['overall_similarity'], reverse=True):
                    f.write(f"é¢˜ç›®: {pair['problem']}\n")
                    f.write(f"æ–‡ä»¶1: {pair['file1']}\n")
                    f.write(f"æ–‡ä»¶2: {pair['file2']}\n")
                    f.write(f"ç»¼åˆç›¸ä¼¼åº¦: {pair['overall_similarity']:.3f}\n")
                    f.write(f"åºåˆ—ç›¸ä¼¼åº¦: {pair['sequence_similarity']:.3f}\n")
                    f.write(f"TF-IDFç›¸ä¼¼åº¦: {pair['tfidf_similarity']:.3f}\n")
                    f.write(f"å˜é‡ç›¸ä¼¼åº¦: {pair['variable_similarity']:.3f}\n")
                    f.write(f"å‡½æ•°ç›¸ä¼¼åº¦: {pair['function_similarity']:.3f}\n")
                    f.write(f"ç»“æ„ç›¸ä¼¼åº¦: {pair['structure_similarity']:.3f}\n")
                    f.write("-" * 30 + "\n\n")
            else:
                f.write("âœ… æœªå‘ç°å¯ç–‘æŠ„è¢­å¯¹\n\n")
            
            # å¤„ç†æ¯ä¸ªé¢˜ç›®
            for problem_name, problem_report in report['problems'].items():
                f.write(f"ğŸ“‹ é¢˜ç›®: {problem_name}\n")
                f.write("=" * 60 + "\n")
                f.write(f"æ–‡ä»¶æ€»æ•°: {problem_report['total_files']}\n")
                f.write(f"åˆ†ææ—¶é—´: {problem_report['analysis_time']}\n\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
            
            # æ‰€æœ‰æ–‡ä»¶ç‰¹å¾åˆ†æï¼ˆæ”¾åœ¨æœ€åï¼‰
            f.write("ğŸ“Š æ‰€æœ‰æ–‡ä»¶ç‰¹å¾åˆ†æ:\n")
            f.write("=" * 80 + "\n")
            
            for problem_name, problem_report in report['problems'].items():
                f.write(f"\nğŸ“‹ é¢˜ç›®: {problem_name}\n")
                f.write("-" * 50 + "\n")
                
                for filename, features in problem_report['file_features'].items():
                    f.write(f"\né¢˜ç›®: {problem_name} | æ–‡ä»¶: {filename}\n")
                    f.write(f"ä»£ç é•¿åº¦: {features['length']} å­—ç¬¦\n")
                    f.write(f"ä»£ç è¡Œæ•°: {features['lines']} è¡Œ\n")
                    f.write(f"å˜é‡æ•°é‡: {len(features['variables'])}\n")
                    f.write(f"å‡½æ•°æ•°é‡: {len(features['functions'])}\n")
                    
                    if features['variables']:
                        f.write(f"å˜é‡åˆ—è¡¨: {', '.join(features['variables'][:10])}\n")
                    if features['functions']:
                        f.write(f"å‡½æ•°åˆ—è¡¨: {', '.join(features['functions'][:10])}\n")
                    
                    # æ·»åŠ ç‰¹æ®Šå…³é”®è¯å’Œæ³¨é‡Šæ£€æµ‹ç»“æœ
                    if features['special_keywords']:
                        f.write("âš ï¸ æ£€æµ‹åˆ°ç‰¹æ®Šå…³é”®è¯å’Œæ³¨é‡Š:\n")
                        for keyword, lines in features['special_keywords'].items():
                            f.write(f"  {keyword}:\n")
                            for line in lines:
                                f.write(f"    {line}\n")
                    
                    f.write(f"ä»£ç å“ˆå¸Œ: {features['hash']}\n")
                    f.write("-" * 25 + "\n")
        
        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_file = os.path.join("result", contest_name, f"{contest_name}_plagiarism_data.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"è¯¦ç»†æ•°æ®å·²ä¿å­˜: {json_file}")


def main():
    detector = PlagiarismDetector()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python plagiarism_detector.py <æ¯”èµ›å>")
        print("ä¾‹å¦‚: python plagiarism_detector.py xxxx")
        return
    
    contest_name = sys.argv[1]
    
    # æ£€æµ‹æŒ‡å®šçš„æ¯”èµ›
    print(f"å¼€å§‹æ£€æµ‹æ¯”èµ›: {contest_name}")
    report = detector.detect_plagiarism(contest_name)
    
    if report:
        detector.generate_report(report, contest_name)
        print(f"\nåæŠ„è¢­æ£€æµ‹å®Œæˆï¼æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ result/{contest_name}/ ç›®å½•ä¸‹")
    else:
        print("æ£€æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¯”èµ›åæ˜¯å¦æ­£ç¡®")


if __name__ == "__main__":
    main() 